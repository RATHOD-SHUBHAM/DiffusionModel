# -*- coding: utf-8 -*-
"""SAM_SD.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1Of9gTEV9A6muwToAf80Ew0mBFfmeRqls
"""

!pip install diffusers==0.10.2 transformers scipy ftfy accelerate
!pip install git+https://github.com/facebookresearch/segment-anything.git
!pip install gradio

from PIL import Image
import requests
import torch
from io import BytesIO

from diffusers import StableDiffusionInpaintPipeline

from segment_anything import SamPredictor, sam_model_registry

import gradio as gr

import numpy as np

"""## Todo: Initiate SAM predictor"""

device = 'cuda' if torch.cuda.is_available() else "cpu"
sam_checkpoint = '/content/drive/MyDrive/Colab_Notebooks/DiffusionModel/Sam_StableDiffusion/weights/sam_vit_h_4b8939.pth'
model_type = "vit_h"
sam = sam_model_registry[model_type](checkpoint=sam_checkpoint)
sam.to(device=device)
predictor = SamPredictor(sam)

"""## Authenticate with Hugging Face to access the Stable Diffusion model"""

from huggingface_hub import notebook_login

# Required to get access to stable diffusion model
notebook_login()

"""## # Todo: Diffusion Pipeline"""

pipe = StableDiffusionInpaintPipeline.from_pretrained(
    "runwayml/stable-diffusion-inpainting",
    torch_dtype=torch.float16
)
pipe = pipe.to(device)

"""## # Gradio block"""

selected_pixels = [] # store the pixels from the image on trigger of event

with gr.Blocks() as webApp:
    # Todo: prompt to take user input
    with gr.Row():
        input_img = gr.Image(label = "Input Image")
        mask_img = gr.Image(label = "mask")
        output_img = gr.Image(label="Output Image")

    with gr.Blocks():
        prompt_text = gr.Textbox(lines=1, label = "Prompt")

    with gr.Row():
        submit = gr.Button("submit")

    # Todo: Generate Mask
    # [Gathering Event Data](https://gradio.app/blocks-and-event-listeners/#gathering-event-data)
    def generate_mask(image, evt: gr.SelectData):

        selected_pixels.append(evt.index)

        predictor.set_image(image)
        # Points are input to the model in (x,y) format and come with labels 1 (foreground point) or 0 (background point).
        input_point = np.array(selected_pixels)
        input_label = np.ones(input_point.shape[0])

        masks, scores, logits = predictor.predict(
            point_coords=input_point,
            point_labels=input_label,
            multimask_output=False, # When False, it will return a single mask.
        )
        # this will return: masks.shape  # (number_of_masks) x H x W

        # convert mask to PIL image
        masks = np.logical_not(masks) # if we want to change background and not foreground
        mask = Image.fromarray(masks[0, : , :])

        return mask

    # Todo: Trigger event added below


    # Todo: Text-Guided Image Inpainting [StableDiffusionInpaintPipeline](https://huggingface.co/docs/diffusers/api/pipelines/stable_diffusion/inpaint)
    def SD_inpaint(image, mask, prompt):
        image = Image.fromarray(image)
        mask = Image.fromarray(mask)

        image = image.resize((512, 512))
        mask = mask.resize((512,512))

        output_image = pipe(prompt=prompt, image=image, mask_image=mask).images[0]

        return output_image

    # from the input image create a trigger to generate mask
    input_img.select(generate_mask, [input_img], [mask_img])
    submit.click(
        fn=SD_inpaint,
        inputs=[input_img, mask_img, prompt_text],
        outputs=[output_img],
        api_name="submit"
    )

webApp.launch()